{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vasugi2003/Big-Data-Analytics/blob/main/Transformations_using_PySpark_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpEZUDkiRDFo",
        "outputId": "a5095c89-9b17-4d61-f695-e3d8d73c6470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=376418883169ab34ab8c569c9e6a1e0f1a0bec60a2c832befcdf6b3bfe53131f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVhMLwDFRAlO"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install and import PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "# Step 2: Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"PySparkTransformationsExample\").getOrCreate()\n",
        "# Sample data\n",
        "data = [(\"aravind\", \"DataAnalyst\", 28),\n",
        "        (\"Banu\", \"Analyst\", 22),\n",
        "        (\"Cibi\", \"Manager\", 35),\n",
        "        (\"Devi\", \"Engineer\", 30)]\n",
        "columns = [\"name\", \"job\", \"age\"]\n",
        "\n",
        "# Step 3: Create a DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Register the DataFrame as a temporary SQL table\n",
        "df.createOrReplaceTempView(\"people\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Perform basic transformations using PySpark SQL\n",
        "\n",
        "# Select specific columns\n",
        "selected_df = spark.sql(\"SELECT name, age FROM people\")\n",
        "\n",
        "# Step 5: Show the results of each transformation\n",
        "print(\"Selected Columns:\")\n",
        "selected_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFzd3RBuTgCo",
        "outputId": "4b82a55f-3251-440e-c9ae-d38bf8a62b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Columns:\n",
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|aravind| 28|\n",
            "|   Banu| 22|\n",
            "|   Cibi| 35|\n",
            "|   Devi| 30|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows based on a condition\n",
        "filtered_df = spark.sql(\"SELECT * FROM people WHERE age > 25\")\n",
        "\n",
        "print(\"Filtered Rows:\")\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gj4G9GBTjPU",
        "outputId": "e8963270-2c04-483c-db54-aeb20c8dcff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Rows:\n",
            "+-------+-----------+---+\n",
            "|   name|        job|age|\n",
            "+-------+-----------+---+\n",
            "|aravind|DataAnalyst| 28|\n",
            "|   Cibi|    Manager| 35|\n",
            "|   Devi|   Engineer| 30|\n",
            "+-------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns\n",
        "renamed_df = spark.sql(\"SELECT name AS full_name, job AS occupation FROM people\")\n",
        "print(\"Renamed Columns:\")\n",
        "renamed_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaX5ROFPTmsp",
        "outputId": "3e227bfb-1f0d-4036-af89-81824597af8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed Columns:\n",
            "+---------+-----------+\n",
            "|full_name| occupation|\n",
            "+---------+-----------+\n",
            "|  aravind|DataAnalyst|\n",
            "|     Banu|    Analyst|\n",
            "|     Cibi|    Manager|\n",
            "|     Devi|   Engineer|\n",
            "+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a new column (e.g., calculating a new age)\n",
        "new_age_df = spark.sql(\"SELECT *, age + 5 AS new_age FROM people\")\n",
        "print(\"Added New Column (new_age):\")\n",
        "new_age_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p3UuB6STovA",
        "outputId": "c4b70754-0b3b-4882-b8a1-2ec2d7c78dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added New Column (new_age):\n",
            "+-------+-----------+---+-------+\n",
            "|   name|        job|age|new_age|\n",
            "+-------+-----------+---+-------+\n",
            "|aravind|DataAnalyst| 28|     33|\n",
            "|   Banu|    Analyst| 22|     27|\n",
            "|   Cibi|    Manager| 35|     40|\n",
            "|   Devi|   Engineer| 30|     35|\n",
            "+-------+-----------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Aggregation (e.g., finding the average age by job)\n",
        "avg_age_by_job = spark.sql(\"SELECT job, AVG(age) AS avg_age FROM people GROUP BY job\")\n",
        "print(\"Aggregation (Average Age by Job):\")\n",
        "avg_age_by_job.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTGHwBM1Tq1B",
        "outputId": "dff70efa-c2bd-4637-9df9-b14c7ad3747a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregation (Average Age by Job):\n",
            "+-----------+-------+\n",
            "|        job|avg_age|\n",
            "+-----------+-------+\n",
            "|    Analyst|   22.0|\n",
            "|DataAnalyst|   28.0|\n",
            "|   Engineer|   30.0|\n",
            "|    Manager|   35.0|\n",
            "+-----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting by age in descending order\n",
        "sorted_df = spark.sql(\"SELECT * FROM people ORDER BY age DESC\")\n",
        "print(\"Sorted by Age:\")\n",
        "sorted_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUOJSZsgTs7C",
        "outputId": "f1e39515-27d1-4e1b-dd25-152bd2d1a141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted by Age:\n",
            "+-------+-----------+---+\n",
            "|   name|        job|age|\n",
            "+-------+-----------+---+\n",
            "|   Cibi|    Manager| 35|\n",
            "|   Devi|   Engineer| 30|\n",
            "|aravind|DataAnalyst| 28|\n",
            "|   Banu|    Analyst| 22|\n",
            "+-------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distinct values in the job column\n",
        "distinct_jobs_df = spark.sql(\"SELECT DISTINCT job FROM people\")\n",
        "print(\"Distinct Jobs:\")\n",
        "distinct_jobs_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi6_yPQuTu-B",
        "outputId": "9773d509-17fd-4970-e8e7-ace3490b9c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct Jobs:\n",
            "+-----------+\n",
            "|        job|\n",
            "+-----------+\n",
            "|    Analyst|\n",
            "|DataAnalyst|\n",
            "|   Engineer|\n",
            "|    Manager|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining two DataFrames based on a common column (job)\n",
        "data2 = [(\"Engineer\", \"Python\"),\n",
        "         (\"Analyst\", \"SQL\")]\n",
        "columns2 = [\"job\", \"skill\"]\n",
        "df2 = spark.createDataFrame(data2, columns2)\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KUxJNsOTxOR",
        "outputId": "8b748c5e-4f9b-41e1-e715-eaa6af6675c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+\n",
            "|     job| skill|\n",
            "+--------+------+\n",
            "|Engineer|Python|\n",
            "| Analyst|   SQL|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.createOrReplaceTempView(\"people_1\")\n",
        "joined_df = spark.sql(\"SELECT * FROM people JOIN people_1 ON people.job = people_1.job\")\n",
        "\n",
        "print(\"Joined DataFrames:\")\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOXVz1mPSRa0",
        "outputId": "48da0aca-f64a-4ebb-87a6-ca335f578f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joined DataFrames:\n",
            "+----+--------+---+--------+------+\n",
            "|name|     job|age|     job| skill|\n",
            "+----+--------+---+--------+------+\n",
            "|Banu| Analyst| 22| Analyst|   SQL|\n",
            "|Devi|Engineer| 30|Engineer|Python|\n",
            "+----+--------+---+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "TgF6sfvGSLd9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}