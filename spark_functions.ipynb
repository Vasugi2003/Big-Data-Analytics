{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT8y/+uwFLfZ6BX7TODmug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vasugi2003/Big-Data-Analytics/blob/main/spark_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP9MSWe-3PdO",
        "outputId": "c075e7b4-daa5-4231-f97e-c1f026f7034c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=93bbc25e35a0b0f8813cc3d09bd762643a4f33643af6c2738b46b0938c84c7ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, col, concat_ws\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WordCountAndFrequency\").getOrCreate()\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "text_file = \"char.txt\"\n",
        "df = spark.read.text(text_file)\n",
        "\n",
        "# Split the lines into words and create a new DataFrame with one word per row\n",
        "words_df = df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
        "\n",
        "# Calculate the total number of words\n",
        "total_word_count = words_df.count()\n",
        "\n",
        "# Create a DataFrame with 3-word combinations\n",
        "window_spec = Window.orderBy(\"word\")\n",
        "combinations_df = words_df.withColumn(\"lag1\", lag(\"word\").over(window_spec))\n",
        "combinations_df = combinations_df.withColumn(\"lag2\", lag(\"lag1\").over(window_spec))\n",
        "combinations_df = combinations_df.filter(col(\"lag2\").isNotNull())\n",
        "\n",
        "# Concatenate the 3 words to form combinations\n",
        "combinations_df = combinations_df.withColumn(\"combination\", concat_ws(\" \", \"lag2\", \"lag1\", \"word\"))\n",
        "\n",
        "# Calculate the frequency of each 3-word combination\n",
        "combination_counts = combinations_df.groupBy(\"combination\").count()\n",
        "\n",
        "# Show the total word count and combination frequencies\n",
        "print(f\"Total word count: {total_word_count}\")\n",
        "print(\"Frequency of 3-word combinations:\")\n",
        "combination_counts.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc_xPzh73hv2",
        "outputId": "aaaa1f4a-cd71-4604-e9e4-ebd6a5362762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count: 14\n",
            "Frequency of 3-word combinations:\n",
            "+------------------+-----+\n",
            "|       combination|count|\n",
            "+------------------+-----+\n",
            "| all everyone glad|    1|\n",
            "|everyone glad good|    1|\n",
            "|    glad good guys|    1|\n",
            "|   good guys hello|    1|\n",
            "|    guys hello hii|    1|\n",
            "|     hello hii iam|    1|\n",
            "|      hii iam meet|    1|\n",
            "|  iam meet morning|    1|\n",
            "|   meet morning to|    1|\n",
            "|  morning to today|    1|\n",
            "|  to today welcome|    1|\n",
            "| today welcome you|    1|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, col, concat_ws, lag\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WordCountAndFrequency\").getOrCreate()\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "text_file = \"char.txt\"\n",
        "df = spark.read.text(text_file)\n",
        "\n",
        "# Split the lines into words and create a new DataFrame with one word per row\n",
        "words_df = df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
        "\n",
        "# Calculate the total number of words\n",
        "total_word_count = words_df.count()\n",
        "\n",
        "# Create a DataFrame with 3-word combinations\n",
        "window_spec = Window.orderBy(\"word\")\n",
        "combinations_df = words_df.withColumn(\"lag1\", lag(\"word\").over(window_spec))\n",
        "combinations_df = combinations_df.withColumn(\"lag2\", lag(\"lag1\").over(window_spec))\n",
        "combinations_df = combinations_df.filter(col(\"lag2\").isNotNull())\n",
        "\n",
        "# Concatenate the 3 words to form combinations\n",
        "combinations_df = combinations_df.withColumn(\"combination\", concat_ws(\" \", \"lag2\", \"lag1\", \"word\"))\n",
        "\n",
        "# Calculate the frequency of each 3-word combination\n",
        "combination_counts = combinations_df.groupBy(\"combination\").count()\n",
        "\n",
        "# Show the total word count and combination frequencies\n",
        "print(f\"Total word count: {total_word_count}\")\n",
        "print(\"Frequency of 3-word combinations:\")\n",
        "combination_counts.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "CviWuFOe9LWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d98852df-fd0c-4103-f062-181e66e6b876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count: 14\n",
            "Frequency of 3-word combinations:\n",
            "+------------------+-----+\n",
            "|       combination|count|\n",
            "+------------------+-----+\n",
            "| all everyone glad|    1|\n",
            "|everyone glad good|    1|\n",
            "|    glad good guys|    1|\n",
            "|   good guys hello|    1|\n",
            "|    guys hello hii|    1|\n",
            "|     hello hii iam|    1|\n",
            "|      hii iam meet|    1|\n",
            "|  iam meet morning|    1|\n",
            "|   meet morning to|    1|\n",
            "|  morning to today|    1|\n",
            "|  to today welcome|    1|\n",
            "| today welcome you|    1|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7B1MjpwLUEne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, concat_ws, col\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WordCountAndFrequency\").getOrCreate()\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "text_file = \"char.txt\"\n",
        "df = spark.read.text(text_file)\n",
        "\n",
        "# Split the lines into words and create a new DataFrame with one word per row\n",
        "words_df = df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
        "\n",
        "# Create a DataFrame with 3-word combinations\n",
        "window_spec = Window.orderBy(\"word\")\n",
        "combinations_df = words_df.withColumn(\"lag1\", lag(\"word\").over(window_spec))\n",
        "combinations_df = combinations_df.withColumn(\"lag2\", lag(\"lag1\").over(window_spec))\n",
        "combinations_df = combinations_df.filter(col(\"lag2\").isNotNull())\n",
        "\n",
        "# Concatenate the 3 words to form combinations\n",
        "combinations_df = combinations_df.withColumn(\"combination\", concat_ws(\" \", \"lag2\", \"lag1\", \"word\"))\n",
        "\n",
        "# Calculate the frequency of each 3-word combination\n",
        "combination_counts = combinations_df.groupBy(\"combination\").count()\n",
        "\n",
        "# Show the 3-word combinations and their count\n",
        "combination_counts.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqXIo6nvUEpw",
        "outputId": "114575a6-793c-4a43-f5d4-50bc2a4c4c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|       combination|count|\n",
            "+------------------+-----+\n",
            "| all everyone glad|    1|\n",
            "|everyone glad good|    1|\n",
            "|    glad good guys|    1|\n",
            "|   good guys hello|    1|\n",
            "|    guys hello hii|    1|\n",
            "|     hello hii iam|    1|\n",
            "|      hii iam meet|    1|\n",
            "|  iam meet morning|    1|\n",
            "|   meet morning to|    1|\n",
            "|  morning to today|    1|\n",
            "|  to today welcome|    1|\n",
            "| today welcome you|    1|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, concat_ws, col\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"WordCountAndFrequency\").getOrCreate()\n",
        "\n",
        "# Read the text file into a DataFrame\n",
        "text_file = \"char.txt\"\n",
        "df = spark.read.text(text_file)\n",
        "\n",
        "# Split the lines into words and create a new DataFrame with one word per row\n",
        "words_df = df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
        "\n",
        "# Create a DataFrame with 3-word combinations, ensuring no consecutive words are grouped together\n",
        "window_spec = Window.orderBy(\"word\")\n",
        "combinations_df = words_df.withColumn(\"lag1\", lag(\"word\").over(window_spec))\n",
        "combinations_df = combinations_df.withColumn(\"lag2\", lag(\"lag1\").over(window_spec))\n",
        "combinations_df = combinations_df.filter(\n",
        "    (col(\"lag2\").isNotNull()) & (col(\"lag2\") != col(\"word\"))\n",
        ")\n",
        "\n",
        "# Concatenate the 3 words to form combinations\n",
        "combinations_df = combinations_df.withColumn(\"combination\", concat_ws(\" \", \"lag2\", \"lag1\", \"word\"))\n",
        "\n",
        "# Calculate the frequency of each 3-word combination\n",
        "combination_counts = combinations_df.groupBy(\"combination\").count()\n",
        "\n",
        "# Show the 3-word combinations and their count\n",
        "combination_counts.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ljp33jzUEtO",
        "outputId": "aff93e08-c365-4c28-9b75-6cccc9794e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|       combination|count|\n",
            "+------------------+-----+\n",
            "| all everyone glad|    1|\n",
            "|everyone glad good|    1|\n",
            "|    glad good guys|    1|\n",
            "|   good guys hello|    1|\n",
            "|    guys hello hii|    1|\n",
            "|     hello hii iam|    1|\n",
            "|      hii iam meet|    1|\n",
            "|  iam meet morning|    1|\n",
            "|   meet morning to|    1|\n",
            "|  morning to today|    1|\n",
            "|  to today welcome|    1|\n",
            "| today welcome you|    1|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZwGX-32l4MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, size\n",
        "spark = SparkSession.builder.appName(\"AverageWordFrequency\").getOrCreate()\n",
        "\\\n",
        "text_file = \"/content/char.txt\"\n",
        "df = spark.read.text(text_file)\n",
        "word_counts_df = df.withColumn(\"words\", split(df[\"value\"], \" \"))\n",
        "word_counts_df = word_counts_df.withColumn(\"word_count\", size(word_counts_df[\"words\"]))\n",
        "\n",
        "average_word_count_df = word_counts_df.groupBy().avg(\"word_count\")\n",
        "\n",
        "average_word_count_df.show()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKpjsYQxl4PJ",
        "outputId": "481856bb-7df8-4292-b503-980695484487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|avg(word_count)|\n",
            "+---------------+\n",
            "|           14.0|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XdHBpaN_l4SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LoTMvfttl4U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-mb2xGNl4X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JaUWGHSTl4bZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}